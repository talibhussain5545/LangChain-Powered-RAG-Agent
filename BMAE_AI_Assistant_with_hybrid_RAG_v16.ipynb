{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dodeeric/langchain-ai-assistant-with-hybrid-rag/blob/main/BMAE_AI_Assistant_with_hybrid_RAG_v16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN1I_30g_aFm"
      },
      "source": [
        "# AI Assistant (LLM Chatbot) with Hybrid RAG -- With chat history\n",
        "v1: Hybrid RAG: keyword search (bm25) and semantic search (vector db)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6CJhAe1Xfph"
      },
      "source": [
        "v2: With memory: 1) Reformulate the question for RAG query (contextualize_q_prompt);  2) Add previous Q and A in prompt sent to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FGgXs38nDgM"
      },
      "source": [
        "v3: With PDF indexation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "v5: With limited number of messages in chat history"
      ],
      "metadata": {
        "id": "cMjC6NpGxw_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kei890_wuANp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet jq bs4 langchain langchain-community langchain-openai langchain-chroma langchainhub rank_bm25 pypdf\n",
        "\n",
        "import requests, json, jq, time, bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader, JSONLoader, PyPDFLoader\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") # To access OpenAI LLM and embedding model via API\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\") # To trace Langchain on Langsmith\n",
        "\n",
        "%env OPENAI_API_KEY = $OPENAI_API_KEY\n",
        "%env LANGCHAIN_API_KEY = $LANGCHAIN_API_KEY\n",
        "%env LANGCHAIN_TRACING_V2 = \"true\"\n",
        "\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm_zLVsh-276"
      },
      "source": [
        "## Scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JQm75HI5g5e0"
      },
      "outputs": [],
      "source": [
        "# Function to scrape the text and the metadata of a web page\n",
        "\n",
        "def scrape_web_page(url):\n",
        "    \"\"\"\n",
        "    Name: swp\n",
        "    Scrape the text and the metadata of a web page\n",
        "    Input: URL of the page\n",
        "    Output: list of dictionaries with: url: url, metadata: metadata, text: text\n",
        "    \"\"\"\n",
        "\n",
        "    #filter = \"two-third last\" # balat / irpa\n",
        "    #filter = \"media-body\" # belgica / kbr\n",
        "    filter = \"hproduct commons-file-information-table\" # commons / wikimedia: summary or description section\n",
        "\n",
        "    # Get the page content\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=(url,),\n",
        "        bs_kwargs=dict(\n",
        "            parse_only=bs4.SoupStrainer(\n",
        "                class_=(filter)\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "    text = loader.load()\n",
        "    # Covert Document type into string type\n",
        "    text = text[0].page_content\n",
        "\n",
        "    # Get the metadata (open graph from Facebook, og:xxx)\n",
        "    # Get the HTML code\n",
        "    response = requests.get(url)\n",
        "    # Transform the HTML code from a Response object type into a BeautifulSoup object type to be scraped by Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Get the metadata fields\n",
        "    metadata = {} # Empty dictionary\n",
        "    # Find all the meta tags in the HTML\n",
        "    meta_tags = soup.find_all(\"meta\")\n",
        "    # Loop through the meta tags\n",
        "    for tag in meta_tags:\n",
        "        property = tag.get(\"property\")\n",
        "        content = tag.get(\"content\")\n",
        "        # Add the property-content pair to the dictionary\n",
        "        if property and content:\n",
        "            metadata[property] = content\n",
        "\n",
        "    # Build JSON string with: url: url, metadata: metadata, text: summary text\n",
        "    # Create a dictionary\n",
        "    page = {\n",
        "        \"url\": url, # String\n",
        "        \"metadata\": metadata, # Dictionary\n",
        "        \"text\": text # String\n",
        "    }\n",
        "\n",
        "    return page # Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX4Rr4Lr8-2M"
      },
      "outputs": [],
      "source": [
        "# METHOD 1: Scrape the URLs from a file and save the results in a JSON file\n",
        "\n",
        "#ds1:\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/balat-urls-ds1\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/belgica-urls-ds1\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/commons-urls-ds1\"\n",
        "\n",
        "#ds2:\n",
        "#file_path = \"/content/drive/MyDrive/colab/balat-urls-ds2\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/commons-urls-ds2\"\n",
        "\n",
        "with open(f\"{file_path}.txt\", \"r\") as urls_file:\n",
        "    items = []\n",
        "    for line in urls_file:\n",
        "        url = line.strip() # Remove spaces at the beginning and at the end of the string\n",
        "        url = url.replace(\"\\ufeff\", \"\")  # Remove BOM (Byte order mark at the start of a text stream)\n",
        "        item = scrape_web_page(url)\n",
        "        print(item)\n",
        "        items.append(item)\n",
        "        #time.sleep(1)\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "# json.dump is designed to take the Python objects, not the already-JSONified string. Read docs.python.org/3/library/json.html.\n",
        "with open(f\"{file_path}-swp.json\", \"w\") as json_file:\n",
        "    json.dump(items, json_file) # That step replaces the accentuated characters (ex: é) by its utf8 codes (ex: \\u00e9)\n",
        "json_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD 2: For Belgica: Scrape the URLs automatically generated and save the results in a JSON file\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/belgica-\"\n",
        "\n",
        "items = []\n",
        "\n",
        "number1 = 10000101\n",
        "step = 100\n",
        "number2 = number1 + step\n",
        "\n",
        "for number in range(number1, number2):\n",
        "    url = f\"https://opac.kbr.be/LIBRARY/doc/SYRACUSE/{number}\"\n",
        "    #print(url)\n",
        "    item = scrape_web_page(url)\n",
        "    print(item)\n",
        "    if item[\"text\"]:\n",
        "        items.append(item)\n",
        "        print(\"saved\")\n",
        "    #time.sleep(1)\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "# json.dump is designed to take the Python objects, not the already-JSONified string. Read docs.python.org/3/library/json.html.\n",
        "with open(f\"{file_path}-{number1}-{number2}-swp.json\", \"w\") as json_file:\n",
        "    json.dump(items, json_file) # That step replaces the accentuated characters (ex: é) by its utf8 codes (ex: \\u00e9)\n",
        "json_file.close()"
      ],
      "metadata": {
        "id": "NdwxOiNwIDf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD 3: For Commons: Scrape the URLs from a Commons Category and save the results in a JSON file\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/commons-\"\n",
        "\n",
        "category = \"Category:Twenty-fifth_wedding_anniversary_of_King_Leopold_II_of_Belgium_and_Queen_Marie-Henriette_in_1878\"\n",
        "\n",
        "items = []\n",
        "href_old = \"\"\n",
        "\n",
        "# Step 1: Load the HTML content from a webpage\n",
        "url = f\"https://commons.wikimedia.org/wiki/{category}\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# Step 2: Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Step 3: Find all URLs in <a> tags\n",
        "urls = []\n",
        "for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    if href:\n",
        "        #print(href)\n",
        "        if href.startswith(\"/wiki/File:\") and href != href_old: # This test because all links are in double!\n",
        "            urls.append(f\"https://commons.wikimedia.org{href}\")\n",
        "            href_old = href\n",
        "\n",
        "#print(\"***********************************************************\")\n",
        "# Print all found URLs\n",
        "#for url in urls:\n",
        "#    print(url)\n",
        "#print(\"***********************************************************\")\n",
        "\n",
        "number_of_pages = len(urls)\n",
        "print(f\"Number of pages to scrape: {number_of_pages}\")\n",
        "\n",
        "i = 1\n",
        "items = []\n",
        "for url in urls:\n",
        "    print(f\"{i}/{number_of_pages}\")\n",
        "    url = url.replace(\"\\ufeff\", \"\")  # Remove BOM (Byte order mark at the start of a text stream)\n",
        "    item = scrape_web_page(url)\n",
        "    print(item)\n",
        "    items.append(item)\n",
        "    #time.sleep(1)\n",
        "    i = i + 1\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "# json.dump is designed to take the Python objects, not the already-JSONified string. Read docs.python.org/3/library/json.html.\n",
        "with open(f\"{file_path}{category}-swp.json\", \"w\") as json_file:\n",
        "    json.dump(items, json_file) # That step replaces the accentuated characters (ex: é) by its utf8 codes (ex: \\u00e9)\n",
        "json_file.close()"
      ],
      "metadata": {
        "id": "Fp9Kn92GaQ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD 4: Scrape one URL and save the result in a JSON file\n",
        "\n",
        "url = \"https://www.europeana.eu/en/item/2048001/AP_10293193\"\n",
        "\n",
        "# Step 1: Load the HTML content from a webpage\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# Step 2: Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "url = url.replace(\"\\ufeff\", \"\")  # Remove BOM (Byte order mark at the start of a text stream)\n",
        "item = scrape_web_page(url)\n",
        "print(item)\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "# json.dump is designed to take the Python objects, not the already-JSONified string. Read docs.python.org/3/library/json.html.\n",
        "with open(f\"temp-swp.json\", \"w\") as json_file:\n",
        "    json.dump(item, json_file) # That step replaces the accentuated characters (ex: é) by its utf8 codes (ex: \\u00e9)\n",
        "json_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1w8CFFkvwTA",
        "outputId": "0d14a50a-0049-4623-8744-20449fe0fec2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'url': 'https://www.europeana.eu/en/item/2048001/AP_10293193', 'metadata': {'og:locale': 'en_GB', 'og:locale:alternate': 'sv_SE', 'og:description': 'Europeana', 'og:url': 'https://www.europeana.eu/en/item/2048001/AP_10293193', 'og:title': 'Portrait du roi Léopold Ier de Belgique', 'og:type': 'article', 'og:image': 'https://api.europeana.eu/thumbnail/v3/400/0295183eeaab1ea54d568fc5432c2d2f'}, 'text': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXt8lI13j5JA"
      },
      "outputs": [],
      "source": [
        "# Open the JSON file to check its content (will produce an error if it's not a correctly formated JSON file)\n",
        "with open(f\"{file_path}-swp.json\", \"r\") as input_file:\n",
        "    items_read = json.load(input_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRAdR5K-uQq"
      },
      "source": [
        "## Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "PR-uONuCr7iu"
      },
      "outputs": [],
      "source": [
        "# Open the JSON files and load each JSON item one by one in the \"documents\" variable (type: Document)\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/colab/commons-urls-ds1-swp.json\"\n",
        "file_path2 = \"/content/drive/MyDrive/colab/balat-urls-ds1-swp.json\"\n",
        "file_path3 = \"/content/drive/MyDrive/colab/belgica-urls-ds1-swp.json\"\n",
        "file_path4 = \"/content/drive/MyDrive/colab/commons-urls-ds2-swp.json\"\n",
        "file_path5 = \"/content/drive/MyDrive/colab/balat-urls-ds2-swp.json\"\n",
        "file_paths = [file_path1, file_path2, file_path3, file_path4, file_path5]\n",
        "\n",
        "documents = []\n",
        "for file_path in file_paths:\n",
        "    loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
        "    docs = loader.load() # Chunks (JSON items) from the JSON files; list of Documents\n",
        "    documents = documents + docs # This variable contents all the JSON items; list of Document"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents[0])"
      ],
      "metadata": {
        "id": "Ral8ZAz6O1eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "FStUm9RgGy4J"
      },
      "outputs": [],
      "source": [
        "# Open the PDF files and load each page one by one in the \"documents\" variable (type: Document)\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/colab/cdf-fxw.pdf\"\n",
        "file_paths = [file_path1]\n",
        "\n",
        "for file_path in file_paths:\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load_and_split() # 1 pdf page per chunk\n",
        "    documents = documents + pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2kNvVf1VVQ7"
      },
      "source": [
        "Run step 1 or step 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WtSS9Dz2Qkwn"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Instanciate a Chroma DB and load the data from disk.\n",
        "collection_name = \"bmae\"\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 3072 dimensions vectors used to embed the chunks and the questions\n",
        "vector_db = Chroma(embedding_function=embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb\")\n",
        "docs = vector_db.get()\n",
        "documents= docs[\"documents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjedA55QYeNS"
      },
      "outputs": [],
      "source": [
        "# STEP 2: ONLY TO EMBED! Instantiate a Chroma DB, embed the JSON items (documents), then save to disk.\n",
        "collection_name = \"bmae\"\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 3072 dimensions vectors used to embed the chunks and the questions\n",
        "vector_db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0gOfeiro_t"
      },
      "source": [
        "## Retrieve and generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EmX4A5jay8WO"
      },
      "outputs": [],
      "source": [
        "# LLM chatbot with a hybrid RAG chain:\n",
        "# (To embed the question, the same model is used as for the data; the model is given in \"vector_db\".)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0)\n",
        "\n",
        "# Semantic search (vector retriever)\n",
        "vector_retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # Chroma DB\n",
        "\n",
        "# Keyword search (bm25 retriever)\n",
        "keyword_retriever = BM25Retriever.from_texts(documents)\n",
        "keyword_retriever.k = 3\n",
        "\n",
        "# Ensemble retriever (mix of both retrivers) -- Weights = order of the results!!! [1,0] means: all bm25 first, all vector after...\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[keyword_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "\"\"\"\n",
        "# Without memory:\n",
        "\n",
        "# Download prompt template: system prompt + inputs (rag_output + chat_history + question)\n",
        "prompt = hub.pull(\"dodeeric/rag-prompt-bmae-with-history\")\n",
        "\n",
        "# Take the text content of each doc, and concatenate them in one string to pass to the prompt (context)\n",
        "def format_docs_clear_text(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content.encode('utf-8').decode('unicode_escape') for doc in docs)\n",
        "\n",
        "# Function to display the text content of the prompt in ai_assistant_chain\n",
        "def print_and_pass(data):\n",
        "    print(f\"Prompt content sent to the LLM: {data}\")\n",
        "    return data\n",
        "\n",
        "# Langchain chain: the LLM chatbot with hybrid RAG. Type: RunnableSequence (chain) -- How/where is the question pass to the RAG??? In LangSmith, we can see the input (question) of the 3 retreivers\n",
        "ai_assistant_chain = ({\"rag_output\": ensemble_retriever | format_docs_clear_text, \"chat_history\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    #| print_and_pass\n",
        "    | llm\n",
        "    | StrOutputParser() # Convert to string\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "chat_history = []\n",
        "chat_history2 = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"\n",
        "Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, ensemble_retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "qa_system_prompt = \"\"\"\n",
        "You are an artwork specialist. You must assist the users in finding, describing, and displaying artworks related to the Belgian monarchy. \\\n",
        "You first have to search answers in the \"Knowledge Base\". If no answers are found in the \"Knowledge Base\", then answer with your own knowledge. \\\n",
        "You have to answer in the same language as the question.\n",
        "At the end of the answer:\n",
        "- give a link to a web page about the artwork (see the \"url\" field).\n",
        "- display an image of the artwork (see the \"og:image\" field).\n",
        "\n",
        "Knowledge Base:\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "ai_assistant_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Vs5q4pAiWZ"
      },
      "source": [
        "Query the AI Assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5iMV6D16T5y8"
      },
      "outputs": [],
      "source": [
        "question = \"Pouvez-vous me montrer le tableau 'La revue des écoles' ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkeT6gTMlpZ9"
      },
      "outputs": [],
      "source": [
        "question = \"Qui a peint ce tableau ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LESYfyp0rs4V"
      },
      "outputs": [],
      "source": [
        "question = \"Quelle est la dimension du tableau ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqH2WKWeaqO5"
      },
      "outputs": [],
      "source": [
        "question = \"Pouvez-vous me montrer un tableau de Charles Porion ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM_Wj8zvb8UR"
      },
      "outputs": [],
      "source": [
        "question = \"Quel est la date de naissance du peintre ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu7YZfAYE-CZ"
      },
      "outputs": [],
      "source": [
        "question = \"Camille Van Camp a-t-il fait des croquis pour sa peinture 'La fête patriotique ' ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVCnHqzq0jhQ"
      },
      "outputs": [],
      "source": [
        "#answer = ai_assistant_chain.invoke(question) # Without memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "djaCokNX46hX"
      },
      "outputs": [],
      "source": [
        "output = ai_assistant_chain.invoke({\"input\": question, \"chat_history\": chat_history}) # output is a dictionary. output[\"answer\"] is in markdown format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha7DuSyBxBkQ"
      },
      "outputs": [],
      "source": [
        "print(output[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history2.save_context({\"input\": question}, {\"output\": output[\"answer\"]})\n",
        "load_memory = chat_history2.load_memory_variables({})\n",
        "chat_history = load_memory[\"history\"]"
      ],
      "metadata": {
        "id": "j63rA40f1PqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O1cK9VlxO05"
      },
      "outputs": [],
      "source": [
        "print(chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrFCZTwc1sMU"
      },
      "outputs": [],
      "source": [
        "# Query the vector RAG only\n",
        "docs = vector_db.similarity_search(question, k=2) # List of Documents; page_content of a Document: string\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests RDF/XML"
      ],
      "metadata": {
        "id": "6u5JHCm4-0fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet rdflib"
      ],
      "metadata": {
        "id": "FWi6a6ykRdWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "from langchain.schema import Document\n",
        "\n",
        "g = Graph()\n",
        "\n",
        "g.parse(\"/content/drive/MyDrive/colab/AP_10093297.xml\", format=\"xml\")\n",
        "\n",
        "# Search image url\n",
        "for index, (sub, pred, obj) in enumerate(g):\n",
        "    if sub.startswith(\"http://balat.kikirpa.be/image/thumbnail/\") and (\"image/jpeg\" in obj):\n",
        "        og_image = sub\n",
        "\n",
        "# Search image page url and image details\n",
        "query = \"\"\"\n",
        "SELECT ?s ?title ?creator ?date ?description\n",
        "WHERE {\n",
        "  ?s <http://purl.org/dc/elements/1.1/title> ?title.\n",
        "  OPTIONAL { ?s <http://purl.org/dc/elements/1.1/date> ?date. }\n",
        "  OPTIONAL { ?s <http://purl.org/dc/elements/1.1/description> ?description. }\n",
        "  OPTIONAL { ?s <http://purl.org/dc/elements/1.1/creator> ?creator. }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "for row in g.query(query):\n",
        "    url = row.s\n",
        "    title = row.title\n",
        "    description = row.description if row.description else ''\n",
        "    date = row.date if row.date else ''\n",
        "    creator = row.creator if row.creator else ''\n",
        "    print(f\"url: {url}, Title: {title}, Creator: {creator}, Date: {date}, Description: {description}, og:image: {og_image}\")\n",
        "\n",
        "item = {\n",
        "    \"url\": url,\n",
        "    \"og:image\": og_image,\n",
        "    \"creator\":  creator,\n",
        "    \"date\": date,\n",
        "    \"description\": description\n",
        "}\n",
        "\n",
        "doc = json.dumps(item)\n",
        "\n",
        "document = Document(page_content=doc)\n",
        "\n",
        "liste = []\n",
        "\n",
        "liste.append(document)\n",
        "\n",
        "print(liste)"
      ],
      "metadata": {
        "id": "aeVzrVT9yd0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PoP9rKaOTJt",
        "outputId": "6f8fd941-dcdc-4464-a975-a81aa70e200f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrtOHTznOUsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PqjWPztuHfgm43-aibCJgI-DBcDxgxXW",
      "authorship_tag": "ABX9TyM/qUrzkFkGeh8moXni7dtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}