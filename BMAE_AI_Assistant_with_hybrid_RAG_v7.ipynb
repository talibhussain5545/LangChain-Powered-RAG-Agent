{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dodeeric/langchain-ai-assistant-with-hybrid-rag/blob/main/BMAE_AI_Assistant_with_hybrid_RAG_v7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN1I_30g_aFm"
      },
      "source": [
        "# AI Assistant (LLM Chatbot) with Hybrid RAG\n",
        "Hybrid RAG: keyword search (bm25) and semantic search (vector db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kei890_wuANp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet jq bs4 langchain langchain-community langchain-openai langchain-chroma langchainhub rank_bm25\n",
        "\n",
        "import requests, json, jq, time, bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader, JSONLoader\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") # To access OpenAI LLM and embedding model via API\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\") # To trace Langchain on Langsmith\n",
        "\n",
        "%env OPENAI_API_KEY = $OPENAI_API_KEY\n",
        "%env LANGCHAIN_API_KEY = $LANGCHAIN_API_KEY\n",
        "%env LANGCHAIN_TRACING_V2 = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm_zLVsh-276"
      },
      "source": [
        "## Scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQm75HI5g5e0"
      },
      "outputs": [],
      "source": [
        "# Function to scrape the text and the metadata of a web page\n",
        "\n",
        "def scrape_web_page(url):\n",
        "    \"\"\"\n",
        "    Name: swp\n",
        "    Scrape the text and the metadata of a web page\n",
        "    Input: URL of the page\n",
        "    Output: JSON with: url: url, metadata: metadata, text: text\n",
        "    \"\"\"\n",
        "\n",
        "    #filter = \"two-third last\" # balat / irpa\n",
        "    #filter = \"notice_corps media\" #  belgica / kbr\n",
        "    filter = \"hproduct commons-file-information-table\" # commons / wikimedia: summary or description section\n",
        "\n",
        "    # Get the page content\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=(url,),\n",
        "        bs_kwargs=dict(\n",
        "            parse_only=bs4.SoupStrainer(\n",
        "                class_=(filter)\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "    text = loader.load()\n",
        "    # Covert Document type into string type\n",
        "    text = text[0].page_content\n",
        "\n",
        "    # Get the metadata (open graph from Facebook, og:xxx)\n",
        "    # Get the HTML code\n",
        "    response = requests.get(url)\n",
        "    # Transform the HTML code from a Response object type into a BeautifulSoup object type to be scraped by Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Get the metadata fields\n",
        "    metadata = {} # Empty dictionary\n",
        "    # Find all the meta tags in the HTML\n",
        "    meta_tags = soup.find_all(\"meta\")\n",
        "    # Loop through the meta tags\n",
        "    for tag in meta_tags:\n",
        "        property = tag.get(\"property\")\n",
        "        content = tag.get(\"content\")\n",
        "        # Add the property-content pair to the dictionary\n",
        "        if property and content:\n",
        "            metadata[property] = content\n",
        "\n",
        "    # Build JSON string with: url: url, metadata: metadata, text: summary text\n",
        "    # Create a dictionary\n",
        "    page = {\n",
        "        \"url\": url, # String\n",
        "        \"metadata\": metadata, # Dictionary\n",
        "        \"text\": text # String\n",
        "    }\n",
        "    # Convert the dictionary to a JSON string\n",
        "    page_json = json.dumps(page)\n",
        "    # Convert in clear text (convert codes in text)\n",
        "    #page_json_clear_text = page_json.encode('utf-8').decode('unicode_escape')\n",
        "\n",
        "    return page_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX4Rr4Lr8-2M"
      },
      "outputs": [],
      "source": [
        "# Scrape the URLs and save the results in a JSON file\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/commons-urls-ds1\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/balat-urls-test\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/belgica-urls-test\"\n",
        "\n",
        "with open(f\"{file_path}.txt\", \"r\") as url_file:\n",
        "    data = []\n",
        "    for line in url_file:\n",
        "        url = line.strip()\n",
        "        url = url.replace(\"\\ufeff\", \"\")  # Remove BOM\n",
        "        page_json = scrape_web_page(url)\n",
        "        print(page_json)\n",
        "        data.append(page_json)\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "with open(f\"{file_path}-swp.json\", \"w\") as output_file:\n",
        "        json.dump(data, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXt8lI13j5JA"
      },
      "outputs": [],
      "source": [
        "# Open the JSON file to check its content (will produce an error if it's not a correctly formated JSON file)\n",
        "with open(f\"{file_path}-swp.json\", \"r\") as file:\n",
        "    data_read = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRAdR5K-uQq"
      },
      "source": [
        "## Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPs_aAdrL4lR"
      },
      "source": [
        "Open the JSON files and embed the items in a local Chroma vector DB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-uONuCr7iu"
      },
      "outputs": [],
      "source": [
        "# Open the JSON files and load each JSON item one by one in the \"Documents\" variable\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/colab/commons-urls-ds1-swp.json\"\n",
        "file_path2 = \"/content/drive/MyDrive/colab/balat-ds1c-wcc-cheerio-ex_2024-04-06_09-05-15-262.json\"\n",
        "file_path3 = \"/content/drive/MyDrive/colab/belgica-ds1c-wcc-cheerio-ex_2024-04-06_08-30-26-786.json\"\n",
        "file_paths = [file_path1, file_path2, file_path3]\n",
        "\n",
        "documents = []\n",
        "for file_path in file_paths:\n",
        "    loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
        "    docs = loader.load() # Chunks (JSON items) from the JSON files; list of Documents\n",
        "    documents = documents + docs # This variable contents all the JSON items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjedA55QYeNS"
      },
      "outputs": [],
      "source": [
        "# Instantiate vector_db and embed the JSON items. The db will content all the JSON items.\n",
        "\n",
        "collection_name = \"bmae-json\"\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 3072 dimensions vectors used to embed the JSON items and the questions\n",
        "vector_db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb13\")\n",
        "# To check the Chroma vector db (sqlite3):\n",
        "# $ sqlite3 chroma.sqlite3\n",
        "# sqlite> .tables ===> List of the tables\n",
        "# sqlite> select * from collections; ===> Name of the collection & size of the vectors\n",
        "# sqlite> select * from embeddings; ===> Number of records in the db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0gOfeiro_t"
      },
      "source": [
        "## Retrieve and generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmX4A5jay8WO"
      },
      "outputs": [],
      "source": [
        "# LLM chatbot with a hybrid RAG chain:\n",
        "# (To embed the question, the same model is used as for the data; the model is given in \"vector_db\".)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0)\n",
        "\n",
        "# Semantic search (vector retriever)\n",
        "vector_retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # Chroma DB\n",
        "\n",
        "# Keyword search (bm25 retriever)\n",
        "keyword_retriever = BM25Retriever.from_documents(documents)\n",
        "keyword_retriever.k = 3\n",
        "\n",
        "# Ensemble retriever (mix of both retrivers) -- Weights = order of the results!!! [1,0] means: all bm25 first, all vector after...\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[keyword_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "# Download prompt template (system prompt + context (rag documents) + user question)\n",
        "prompt = hub.pull(\"dodeeric/rag-prompt-bmae\")\n",
        "\n",
        "# Take the text content of each doc, and concatenate them in one string to pass to the prompt (context)\n",
        "def format_docs_clear_text(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content.encode('utf-8').decode('unicode_escape') for doc in docs)\n",
        "\n",
        "# Function to display the text content of the prompt in ai_assistant_chain\n",
        "def print_and_pass(data):\n",
        "    print(f\"Prompt content sent to the LLM: {data}\")\n",
        "    return data\n",
        "\n",
        "# Langchain chain: the LLM chatbot with hybrid RAG:\n",
        "ai_assistant_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs_clear_text, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    #| print_and_pass\n",
        "    | llm\n",
        "    | StrOutputParser() # Convert to string\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Vs5q4pAiWZ"
      },
      "source": [
        "Query the AI Assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iMV6D16T5y8"
      },
      "outputs": [],
      "source": [
        "question = \"Pouvez-vous me montrer le tableau la revue des écoles présent de la base de donnée de BALaT KIK-IRPA, pas celle de la Wikimedia commons ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "djaCokNX46hX",
        "outputId": "99e59553-4647-478a-9a3d-26bf4fe688d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Vous pouvez voir le tableau \"La revue des écoles en 1878 en présence de Léopold II\" sur la base de données de BALaT KIK-IRPA en suivant ce lien : [BALaT KIK-IRPA](https://balat.kikirpa.be/object/130731)\\n\\n![La revue des écoles en 1878](http://balat.kikirpa.be/image/thumbnail/B213530.jpg)'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_assistant_chain.invoke(question) # Output is in markdown format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrFCZTwc1sMU"
      },
      "outputs": [],
      "source": [
        "# Query the vector RAG only\n",
        "docs = vector_db.similarity_search(question, k=2) # List of Documents; page_content of a Document: string\n",
        "rag_context = format_docs_clear_text(docs) # One string composed of k json items\n",
        "print(rag_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLdKbf4vxeac"
      },
      "source": [
        "# Streamlit Web interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xWqD7_KRa2H"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhTnTv5BWYtJ"
      },
      "outputs": [],
      "source": [
        "#!pip install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbc-7Tv528Tl"
      },
      "outputs": [],
      "source": [
        "# The IP is the password\n",
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No8e1571M10w"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "spectra = st.file_uploader(\"upload CSV file\", type={\"csv\"})\n",
        "if spectra is not None:\n",
        "  spectra_df = pd.read_csv(spectra)\n",
        "  st.write(spectra_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFs6ShhaOI4Q"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.title('AI-powered Document Query Assistant')\n",
        "\n",
        "# Text input for the query\n",
        "user_query = st.text_area(\"Enter your query:\", help='Type your query here and press enter.')\n",
        "\n",
        "# Button to submit query\n",
        "if st.button('Search'):\n",
        "    if user_query:\n",
        "        # This is where your AI assistant's processing happens\n",
        "        response = ai_assistant_chain.invoke(user_query)\n",
        "        st.write(response)  # Display the response\n",
        "    else:\n",
        "        st.write(\"Please enter a query to proceed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhxa2DeGM6K3"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17bJ7EVG5mL2KUOJDRNfKbQDYnccI_YBF",
      "authorship_tag": "ABX9TyPoNNE3cnuUvIQRatUqVb8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}