# -*- coding: utf-8 -*-
"""BMAE - AI Assistant with hybrid RAG - v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17bJ7EVG5mL2KUOJDRNfKbQDYnccI_YBF

# AI Assistant (LLM Chatbot) with Hybrid RAG
Hybrid RAG: keyword search (bm25) and semantic search (vector db)
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install --upgrade --quiet jq bs4 langchain langchain-community langchain-openai langchain-chroma langchainhub rank_bm25

import requests, json, jq, time, bs4
from bs4 import BeautifulSoup
from google.colab import userdata
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader, JSONLoader
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

OPENAI_API_KEY = userdata.get("OPENAI_API_KEY")
LANGCHAIN_API_KEY = userdata.get("LANGCHAIN_API_KEY")

# %env OPENAI_API_KEY = $OPENAI_API_KEY
# %env LANGCHAIN_API_KEY = $LANGCHAIN_API_KEY
# %env LANGCHAIN_TRACING_V2 = "true"

"""## Scrape"""

# Scrape the text and the metadata of a web page

def scrape_web_page(url):
    """
    Name: swp
    Scrape the text and the metadata of a web page
    Input: URL of the page
    Output: JSON with: url: url, metadata: metadata, text: text
    """

    #filter = "two-third last" # balat / irpa
    #filter = "notice_corps media" #  belgica / kbr
    filter = "hproduct commons-file-information-table" # commons / wikimedia

    # Get the page content
    loader = WebBaseLoader(
        web_paths=(url,),
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer(
                class_=(filter)
            )
        ),
    )
    text = loader.load()
    # Covert Document type into string type
    text = text[0].page_content

    # Get the metadata (open graph from Facebook, og:xxx)
    # Get the HTML code
    response = requests.get(url)
    # Transform the HTML code from a Response object type into a BeautifulSoup object type to be scraped by Beautiful Soup
    soup = BeautifulSoup(response.text, "html.parser")
    # Get the metadata fields
    metadata = {} # Empty dictionary
    # Find all the meta tags in the HTML
    meta_tags = soup.find_all("meta")
    # Loop through the meta tags
    for tag in meta_tags:
        property = tag.get("property")
        content = tag.get("content")
        # Add the property-content pair to the dictionary
        if property and content:
            metadata[property] = content

    # Build JSON string with: url: url, metadata: metadata, text: summary text
    # Create a dictionary
    page = {
        "url": url, # String
        "metadata": metadata, # Dictionary
        "text": text # String
    }
    # Convert the dictionary to a JSON string
    page_json = json.dumps(page)
    # Convert in clear text (convert codes in text)
    #page_json_clear_text = page_json.encode('utf-8').decode('unicode_escape')

    return page_json

# Scrape the URLs and save the results in a JSON file

file_path = "/content/drive/MyDrive/colab/commons-urls-ds1"
#file_path = "/content/drive/MyDrive/colab/balat-urls-test"
#file_path = "/content/drive/MyDrive/colab/belgica-urls-test"

with open(f"{file_path}.txt", "r") as url_file:
    data = []
    for line in url_file:
        url = line.strip()
        url = url.replace("\ufeff", "")  # Remove BOM
        page_json = scrape_web_page(url)
        print(page_json)
        data.append(page_json)
        time.sleep(1)

# Save the Python list in a JSON file
with open(f"{file_path}-swp.json", "w") as output_file:
        json.dump(data, output_file)

# Open the JSON file to check its content (will produce an error if it's not a correctly formated JSON file)
with open(f"{file_path}-swp.json", "r") as file:
    data_read = json.load(file)

"""## Index

Open the JSON files and embed the items in a local Chroma vector DB.
"""

# Open the JSON file and load each json item one by one in the "Documents" variable

file_path1 = "/content/drive/MyDrive/colab/commons-urls-ds1-swp.json"
file_path2 = "/content/drive/MyDrive/colab/balat-ds1c-wcc-cheerio-ex_2024-04-06_09-05-15-262.json"
file_path3 = "/content/drive/MyDrive/colab/belgica-ds1c-wcc-cheerio-ex_2024-04-06_08-30-26-786.json"
file_path = [file_path1, file_path2, file_path3]

documents = []
for file_p in file_path:
    loader = JSONLoader(file_path=file_p, jq_schema=".[]", text_content=False)
    docs = loader.load() # Chunks (JSON item) from the JSON file; list of Documents
    documents = documents + docs

# To execute only to embedd in the vector db:

collection_name = "bmae-json"
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large") # 3072 dimensions vectors used to embed the JSON items and the questions

vector_db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name, persist_directory="/content/drive/MyDrive/colab/chromadb6")

"""## Retrieve and generate"""

# LLM chatbot with a hybrid RAG chain:
# (To embed the question, the same model is used as for the data; the model is given in "vector_db".)

llm = ChatOpenAI(model="gpt-4-turbo-2024-04-09", temperature=0.1)

# Semantic search (vector retriever)
vector_retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 3}) # Chroma DB

# Keyword search (bm25 retriever)
keyword_retriever = BM25Retriever.from_documents(documents)
keyword_retriever.k = 3

# Ensemble retriever (mix of both retrivers) -- Weights = order of the results!!! [1,0] means: all bm25 first, all vector after...
ensemble_retriever = EnsembleRetriever(retrievers=[keyword_retriever, vector_retriever], weights=[0.5, 0.5])

# Download prompt template (system prompt + context (rag documents) + user question)
prompt = hub.pull("dodeeric/rag-prompt-bmae")

# Take the text content of each doc, and concatenate them in one string to pass to the prompt (context)
def format_docs_clear_text(docs):
    return "\n\n".join(doc.page_content.encode('utf-8').decode('unicode_escape') for doc in docs)

# Function to display the text content of the prompt in ai_assistant_chain
def print_and_pass(data):
    print(f"Prompt content sent to the LLM: {data}")
    return data

ai_assistant_chain = (
    {"context": ensemble_retriever | format_docs_clear_text, "question": RunnablePassthrough()}
    | prompt
    #| print_and_pass
    | llm
    | StrOutputParser() # Convert to string
)

"""Querry the AI Assistant:"""

question = "Pouvez-vous me montrer un tableau peint par Lieven De Winne ?"

ai_assistant_chain.invoke(question)

# Query the vector RAG only
docs = vector_db.similarity_search(question, k=2) # List of Documents; page_content of a Document: string
rag_context = format_docs_clear_text(docs) # One string composed of k json items
print(rag_context)