{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dodeeric/langchain-ai-assistant-with-hybrid-rag/blob/main/BMAE_AI_Assistant_with_hybrid_RAG_v10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN1I_30g_aFm"
      },
      "source": [
        "# AI Assistant (LLM Chatbot) with Hybrid RAG\n",
        "Hybrid RAG: keyword search (bm25) and semantic search (vector db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kei890_wuANp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet jq bs4 langchain langchain-community langchain-openai langchain-chroma langchainhub rank_bm25\n",
        "\n",
        "import requests, json, jq, time, bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader, JSONLoader\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") # To access OpenAI LLM and embedding model via API\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\") # To trace Langchain on Langsmith\n",
        "\n",
        "%env OPENAI_API_KEY = $OPENAI_API_KEY\n",
        "%env LANGCHAIN_API_KEY = $LANGCHAIN_API_KEY\n",
        "%env LANGCHAIN_TRACING_V2 = \"true\"\n",
        "\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm_zLVsh-276"
      },
      "source": [
        "## Scrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQm75HI5g5e0"
      },
      "outputs": [],
      "source": [
        "# Function to scrape the text and the metadata of a web page\n",
        "\n",
        "def scrape_web_page(url):\n",
        "    \"\"\"\n",
        "    Name: swp\n",
        "    Scrape the text and the metadata of a web page\n",
        "    Input: URL of the page\n",
        "    Output: list of dictionaries with: url: url, metadata: metadata, text: text\n",
        "    \"\"\"\n",
        "\n",
        "    #filter = \"two-third last\" # balat / irpa\n",
        "    #filter = \"notice_corps media\" #  belgica / kbr\n",
        "    filter = \"hproduct commons-file-information-table\" # commons / wikimedia: summary or description section\n",
        "\n",
        "    # Get the page content\n",
        "    loader = WebBaseLoader(\n",
        "        web_paths=(url,),\n",
        "        bs_kwargs=dict(\n",
        "            parse_only=bs4.SoupStrainer(\n",
        "                class_=(filter)\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "    text = loader.load()\n",
        "    # Covert Document type into string type\n",
        "    text = text[0].page_content\n",
        "\n",
        "    # Get the metadata (open graph from Facebook, og:xxx)\n",
        "    # Get the HTML code\n",
        "    response = requests.get(url)\n",
        "    # Transform the HTML code from a Response object type into a BeautifulSoup object type to be scraped by Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Get the metadata fields\n",
        "    metadata = {} # Empty dictionary\n",
        "    # Find all the meta tags in the HTML\n",
        "    meta_tags = soup.find_all(\"meta\")\n",
        "    # Loop through the meta tags\n",
        "    for tag in meta_tags:\n",
        "        property = tag.get(\"property\")\n",
        "        content = tag.get(\"content\")\n",
        "        # Add the property-content pair to the dictionary\n",
        "        if property and content:\n",
        "            metadata[property] = content\n",
        "\n",
        "    # Build JSON string with: url: url, metadata: metadata, text: summary text\n",
        "    # Create a dictionary\n",
        "    page = {\n",
        "        \"url\": url, # String\n",
        "        \"metadata\": metadata, # Dictionary\n",
        "        \"text\": text # String\n",
        "    }\n",
        "\n",
        "    return page # Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX4Rr4Lr8-2M"
      },
      "outputs": [],
      "source": [
        "# Scrape the URLs and save the results in a JSON file\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/colab/commons-urls-ds1\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/commons-urls-test\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/balat-urls-test\"\n",
        "#file_path = \"/content/drive/MyDrive/colab/belgica-urls-test\"\n",
        "\n",
        "with open(f\"{file_path}.txt\", \"r\") as urls_file:\n",
        "    items = []\n",
        "    for line in urls_file:\n",
        "        url = line.strip()\n",
        "        url = url.replace(\"\\ufeff\", \"\")  # Remove BOM\n",
        "        item = scrape_web_page(url)\n",
        "        print(item)\n",
        "        items.append(item)\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save the Python list in a JSON file\n",
        "# json.dump is designed to take the Python objects, not the already-JSONified string. Read docs.python.org/3/library/json.html.\n",
        "with open(f\"{file_path}-swp.json\", \"w\") as json_file:\n",
        "    json.dump(items, json_file) # Replaces the accentuated characters (ex: Ã©) by its utf8 codes (ex: \\u00e9)\n",
        "json_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXt8lI13j5JA"
      },
      "outputs": [],
      "source": [
        "# Open the JSON file to check its content (will produce an error if it's not a correctly formated JSON file)\n",
        "with open(f\"{file_path}-swp.json\", \"r\") as input_file:\n",
        "    items_read = json.load(input_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRAdR5K-uQq"
      },
      "source": [
        "## Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PR-uONuCr7iu"
      },
      "outputs": [],
      "source": [
        "# Open the JSON files and load each JSON item one by one in the \"documents\" variable (type: Document)\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/colab/commons-urls-ds1-swp.json\"\n",
        "file_path2 = \"/content/drive/MyDrive/colab/balat-ds1c-wcc-cheerio-ex_2024-04-06_09-05-15-262.json\"\n",
        "file_path3 = \"/content/drive/MyDrive/colab/belgica-ds1c-wcc-cheerio-ex_2024-04-06_08-30-26-786.json\"\n",
        "file_paths = [file_path1, file_path2, file_path3]\n",
        "\n",
        "documents = []\n",
        "for file_path in file_paths:\n",
        "    loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
        "    docs = loader.load() # Chunks (JSON items) from the JSON files; list of Documents\n",
        "    documents = documents + docs # This variable contents all the JSON items"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run step 1 or step 2:"
      ],
      "metadata": {
        "id": "K2kNvVf1VVQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Instanciate a Chroma DB and load the data from disk.\n",
        "collection_name = \"bmae-json\"\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 3072 dimensions vectors used to embed the JSON items and the questions\n",
        "vector_db = Chroma(embedding_function=embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb2\")"
      ],
      "metadata": {
        "id": "WtSS9Dz2Qkwn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjedA55QYeNS"
      },
      "outputs": [],
      "source": [
        "# STEP 2: ONLY TO EMBED! Instantiate a Chroma DB, embed the JSON items (documents), then save to disk.\n",
        "collection_name = \"bmae-json\"\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\") # 3072 dimensions vectors used to embed the JSON items and the questions\n",
        "vector_db = Chroma.from_documents(documents, embedding_model, collection_name=collection_name, persist_directory=\"/content/drive/MyDrive/colab/chromadb2\")\n",
        "# To check the Chroma vector db (sqlite3):\n",
        "# $ sqlite3 chroma.sqlite3\n",
        "# sqlite> .tables ===> List of the tables\n",
        "# sqlite> select * from collections; ===> Name of the collection & size of the vectors\n",
        "# sqlite> select * from embeddings; ===> Number of records in the db\n",
        "# sqlite> select * from embedding_metadata; ===> Display json items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0gOfeiro_t"
      },
      "source": [
        "## Retrieve and generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EmX4A5jay8WO"
      },
      "outputs": [],
      "source": [
        "# LLM chatbot with a hybrid RAG chain:\n",
        "# (To embed the question, the same model is used as for the data; the model is given in \"vector_db\".)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0)\n",
        "\n",
        "# Semantic search (vector retriever)\n",
        "vector_retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # Chroma DB\n",
        "\n",
        "# Keyword search (bm25 retriever)\n",
        "keyword_retriever = BM25Retriever.from_documents(documents)\n",
        "keyword_retriever.k = 3\n",
        "\n",
        "# Ensemble retriever (mix of both retrivers) -- Weights = order of the results!!! [1,0] means: all bm25 first, all vector after...\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[keyword_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "# Download prompt template (system prompt + context (rag documents) + user question)\n",
        "prompt = hub.pull(\"dodeeric/rag-prompt-bmae\")\n",
        "\n",
        "# Take the text content of each doc, and concatenate them in one string to pass to the prompt (context)\n",
        "def format_docs_clear_text(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content.encode('utf-8').decode('unicode_escape') for doc in docs)\n",
        "\n",
        "# Function to display the text content of the prompt in ai_assistant_chain\n",
        "def print_and_pass(data):\n",
        "    print(f\"Prompt content sent to the LLM: {data}\")\n",
        "    return data\n",
        "\n",
        "# Langchain chain: the LLM chatbot with hybrid RAG:\n",
        "ai_assistant_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs_clear_text, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    #| print_and_pass\n",
        "    | llm\n",
        "    | StrOutputParser() # Convert to string\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Vs5q4pAiWZ"
      },
      "source": [
        "Query the AI Assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5iMV6D16T5y8"
      },
      "outputs": [],
      "source": [
        "question = \"Can you show me the canvas 'the review of the schools' which is present in the BALaT KIK-IRPA database, not the one from Wikimedia Commons?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "djaCokNX46hX",
        "outputId": "0adbc681-acc0-4751-8141-3513e287afbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Certainly! The painting \"La revue des Ã©coles en 1878\" by Jan Frans Verhas is present in the BALaT KIK-IRPA database. This artwork depicts the review of the communal schools of the city of Brussels, which took place on August 23, 1878, in front of the Royal Palace of Brussels. The event was held in honor of the silver wedding anniversary of King Leopold II and Queen Marie-Henriette. The painting captures a significant moment with various dignitaries and members of the royal family, including King Leopold II, present during the review.\\n\\nFor more details, you can visit the BALaT KIK-IRPA page for this artwork:\\n[La revue des Ã©coles en 1878 - BALaT KIK-IRPA](https://balat.kikirpa.be/object/130731)\\n\\n![La revue des Ã©coles en 1878](http://balat.kikirpa.be/image/thumbnail/B213530.jpg)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "ai_assistant_chain.invoke(question) # Output is in markdown format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrFCZTwc1sMU"
      },
      "outputs": [],
      "source": [
        "# Query the vector RAG only\n",
        "docs = vector_db.similarity_search(question, k=2) # List of Documents; page_content of a Document: string\n",
        "rag_context = format_docs_clear_text(docs) # One string composed of k json items\n",
        "print(rag_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLdKbf4vxeac"
      },
      "source": [
        "# Streamlit Web interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xWqD7_KRa2H"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhTnTv5BWYtJ"
      },
      "outputs": [],
      "source": [
        "#!pip install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbc-7Tv528Tl"
      },
      "outputs": [],
      "source": [
        "# The IP is the password\n",
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No8e1571M10w"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "spectra = st.file_uploader(\"upload CSV file\", type={\"csv\"})\n",
        "if spectra is not None:\n",
        "  spectra_df = pd.read_csv(spectra)\n",
        "  st.write(spectra_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFs6ShhaOI4Q"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.title('AI-powered Document Query Assistant')\n",
        "\n",
        "# Text input for the query\n",
        "user_query = st.text_area(\"Enter your query:\", help='Type your query here and press enter.')\n",
        "\n",
        "# Button to submit query\n",
        "if st.button('Search'):\n",
        "    if user_query:\n",
        "        # This is where your AI assistant's processing happens\n",
        "        response = ai_assistant_chain.invoke(user_query)\n",
        "        st.write(response)  # Display the response\n",
        "    else:\n",
        "        st.write(\"Please enter a query to proceed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhxa2DeGM6K3"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "146BFdEWZZRi3y5sKU3ZddVUYsor1fYRr",
      "authorship_tag": "ABX9TyP+oemjk0exi6D3IusD24In",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}